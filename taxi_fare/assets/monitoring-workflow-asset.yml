# TODO: Add data monitoring support for mlops
#new_cluster: &new_cluster
#  new_cluster:
#    num_workers: 3
#    spark_version: 13.3.x-cpu-ml-scala2.12
#    node_type_id: i3.xlarge
#    custom_tags:
#      clusterSource: mlops-stack/0.2

common_permissions: &permissions
  permissions:
    - level: CAN_VIEW
      group_name: users

resources:
  jobs:
    monitoring_job:
      name: "[${bundle.target}] [${bundle.name}] monitoring-job"
      max_concurrent_runs: 3
      tasks:
        - task_key: create_monitoring
          existing_cluster_id: ${var.cluster_id} #<<: *new_cluster
          notebook_task:
            notebook_path: ../monitoring/notebooks/MonitoringCreation.py
            base_parameters:
              env: ${bundle.target}
              inference_table_name: ${var.catalog_name}.${var.schema_name}${bundle.target}.inference_logs
              baseline_table_name: ${var.catalog_name}.${var.schema_name}${bundle.target}.baseline_logs
              model_name: ${var.catalog_name}.${var.schema_name}${bundle.target}.${var.model_name}
              timestamp_col: inference_timestamp
              model_id_col: model_version
              prediction_col: ${var.prediction_column}
              label_col: ${var.label_column}
              output_schema_name: ${var.catalog_name}.${var.schema_name}${bundle.target}
              reset_monitoring: no_reset
              # git source information of current ML asset deployment. It will be persisted as part of the workflow run
              git_source_info: url:${bundle.git.origin_url}; branch:${bundle.git.branch}; commit:${bundle.git.commit}
        - task_key: simulate_drift
          depends_on:
            - task_key: create_monitoring
          existing_cluster_id: ${var.cluster_id} #<<: *new_cluster
          notebook_task:
            notebook_path: ../monitoring/notebooks/SimulateDrift.py
            base_parameters:
              env: ${bundle.target}
              inference_table_todrift: ${var.catalog_name}.${var.schema_name}${bundle.target}.inference_table_todrift
              inference_table_drifted: ${var.catalog_name}.${var.schema_name}${bundle.target}.inference_table_drifted
              # git source information of current ML asset deployment. It will be persisted as part of the workflow run
              git_source_info: url:${bundle.git.origin_url}; branch:${bundle.git.branch}; commit:${bundle.git.commit}
        - task_key: inference_with_drift
          depends_on:
            - task_key: simulate_drift
          existing_cluster_id: ${var.cluster_id} #<<: *new_cluster
          notebook_task:
            notebook_path: ../deployment/batch_inference/notebooks/BatchInference.py
            base_parameters:
              env: ${bundle.target}
              input_table_name: ${var.catalog_name}.${var.schema_name}${bundle.target}.inference_table_drifted  
              output_table_name: ${var.catalog_name}.${var.schema_name}${bundle.target}.inference_logs
              model_name: ${var.catalog_name}.${var.schema_name}${bundle.target}.${var.model_name}
              # git source information of current ML asset deployment. It will be persisted as part of the workflow run
              git_source_info: url:${bundle.git.origin_url}; branch:${bundle.git.branch}; commit:${bundle.git.commit}
        - task_key: refresh_monitor
          depends_on:
            - task_key: inference_with_drift
          existing_cluster_id: ${var.cluster_id} #<<: *new_cluster
          notebook_task:
            notebook_path: ../monitoring/notebooks/RefreshMonitor.py
            base_parameters:
              env: ${bundle.target}
              inference_table_name: ${var.catalog_name}.${var.schema_name}${bundle.target}.inference_logs
              # git source information of current ML asset deployment. It will be persisted as part of the workflow run
              git_source_info: url:${bundle.git.origin_url}; branch:${bundle.git.branch}; commit:${bundle.git.commit}

      schedule:
        quartz_cron_expression: "0 0 11 * * ?" # daily at 11am
        timezone_id: UTC
      <<: *permissions
      # If you want to turn on notifications for this job, please uncomment the below code,
      # and provide a list of emails to the on_failure argument.
      #
      #  email_notifications:
      #    on_failure:
      #      - first@company.com
      #      - second@company.com
